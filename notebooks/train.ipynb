{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from geo_dist_prep.data import GEONAMES_DB\n",
    "from geo_dist_prep.schemas.training_data import TrainingData\n",
    "\n",
    "# engine = create_engine(f\"sqlite:///../{GEONAMES_DB}\")\n",
    "engine = create_engine(f\"sqlite:///../.cache/geonames.db.britain.with-data\")\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "all_data_q = session.query(TrainingData)\n",
    "\n",
    "all_data = pd.read_sql(all_data_q.statement, all_data_q.session.bind)\n",
    "all_data.head()\n",
    "\n",
    "all_data = all_data[all_data[\"distance\"] < 50]\n",
    "all_data = all_data[all_data[\"distance\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shuff = all_data.sample(10000)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# for _, row in shuff.iterrows():\n",
    "#     for x, y in [(row['x1'], row['y1']), (row['x2'], row['y2'])]:\n",
    "#         # if y < 0.574 or y > .578 or x < .798 or x > .802:\n",
    "#         #     continue\n",
    "\n",
    "#         count += 1\n",
    "\n",
    "plt.hist(all_data[\"distance\"], bins=1000)\n",
    "# plt.scatter(shuff['x1'], shuff['y1'], s=0.5, color='red')\n",
    "# plt.scatter(shuff['x2'], shuff['y2'], s=0.5, color='blue')\n",
    "# y*180, x*360, s=2, color='red' if count % 2 == 0 else 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "\n",
    "from geo_dist_prep.model.visualize import plot_losses\n",
    "\n",
    "from math import sqrt, pow\n",
    "\n",
    "\n",
    "def distance(x1, y1, x2, y2):\n",
    "    dist_y_sq = pow(y1 - y2, 2)\n",
    "    dist_x_sq = pow(x1 - x2, 2)\n",
    "\n",
    "    return sqrt(dist_x_sq + dist_y_sq)\n",
    "\n",
    "\n",
    "from math import atan2, pi, cos, radians\n",
    "from geo_dist_prep.normalize import normalize_coords\n",
    "\n",
    "\n",
    "def get_data(lat1, lat2, lon1, lon2):\n",
    "    y1, x1 = normalize_coords(row[\"lat1\"], row[\"lon1\"])\n",
    "    y2, x2 = normalize_coords(row[\"lat2\"], row[\"lon2\"])\n",
    "    pos = (x1 * y1 + x2 * y2) / 2\n",
    "\n",
    "    delta_lat = row[\"lat2\"] - row[\"lat1\"]\n",
    "    delta_lon = row[\"lon2\"] - row[\"lon1\"]\n",
    "\n",
    "    angle = atan2(delta_lat, delta_lon) + pi\n",
    "    direction = (angle % (2 * pi)) / (2 * pi)\n",
    "\n",
    "    x1_km = row[\"lon1\"] * 111.32 * cos(radians(row[\"lat1\"]))\n",
    "    x2_km = row[\"lon2\"] * 111.32 * cos(radians(row[\"lat2\"]))\n",
    "    y1_km = row[\"lat1\"] * 111.32\n",
    "    y2_km = row[\"lat2\"] * 111.32\n",
    "    dist = distance(x1_km, y1_km, x2_km, y2_km)\n",
    "\n",
    "    return {\n",
    "        \"pos\": pos,\n",
    "        \"direction\": direction,\n",
    "        \"sky_dist\": dist,\n",
    "    }\n",
    "\n",
    "\n",
    "for row in all_data.iterrows():\n",
    "    d = normalize(row[1])\n",
    "    row[d.keys()] = d.values()\n",
    "\n",
    "# all_data.apply(normalize, axis=1, result_type='expand')\n",
    "\n",
    "# all_data[\"pos\"] = (all_data[\"x1\"] * all_data[\"y1\"] + all_data[\"x2\"] * all_data[\"y2\"]) / 2\n",
    "# all_data[\"sky_dist\"] = all_data.apply(lambda row: distance(row['x1'], row['y1'], row['x2'], row['y2']), axis=1)\n",
    "all_data[\"penalty\"] = all_data[\"sky_dist\"] / (all_data[\"distance\"] / 40000)\n",
    "# all_data[\"sky_dist\"] = 1 - 1 / all_data[\"sky_dist\"]\n",
    "# all_data[\"dirmag\"] = all_data[\"sky_dist\"] * all_data[\"direction\"]\n",
    "\n",
    "features = [\n",
    "    \"pos\",\n",
    "    \"direction\",\n",
    "    \"sky_dist\",\n",
    "]\n",
    "input_shape = len(features)\n",
    "predictions = [\"penalty\"]\n",
    "output_shape = len(predictions)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(128, input_shape=(input_shape,)),\n",
    "        Dense(32),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        Dense(32),\n",
    "        LeakyReLU(),\n",
    "        Dense(32),\n",
    "        Dense(output_shape, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "\n",
    "# Split data into training and validation sets\n",
    "data = all_data.copy()\n",
    "# d2  = data.copy()\n",
    "# d2.rename(columns={'x1': 'x2', 'x2': 'x1', 'y1': 'y2', 'y2': 'y1'}, inplace=True)\n",
    "# data = data + d2\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# data = data.query(\n",
    "#    '(0.574 <= lat1 <= 0.578) and (0.798 <= lon1 <= 0.802) and \\\n",
    "#    (0.574 <= lat2 <= 0.578) and (0.798 <= lon2 <= 0.802)'\n",
    "# )\n",
    "\n",
    "train = data.sample(frac=0.8, random_state=0)\n",
    "val = data.drop(train.index)\n",
    "\n",
    "# print(len(train))\n",
    "\n",
    "# data = pd.DataFrame(scaler.fit_transform(data))\n",
    "\n",
    "# Split data into features and labels\n",
    "train_x = train[features]\n",
    "train_y = train[predictions]\n",
    "val_x = val[features]\n",
    "val_y = val[predictions]\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=300,\n",
    "    batch_size=5000,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[plot_losses],\n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "model.evaluate(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "southport = 26700924\n",
    "lat1, lon1 = -3.004175, 53.647599  # southport\n",
    "lat2, lon2 = -2.9979, 50.7463  # lytham\n",
    "# lat2, lon2 = -2.2522, 53.477 # manchester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's try to predict the density of a grid location\n",
    "#\n",
    "from geo_dist_prep.schemas.geoname import GeoName\n",
    "from sqlalchemy import func, Integer, Float\n",
    "import math\n",
    "\n",
    "\n",
    "def lat_to_km(lat: float) -> float:\n",
    "    \"\"\"Converts degrees latitude to kilometers.\"\"\"\n",
    "    return lat * 111.32\n",
    "\n",
    "\n",
    "def lon_to_km(lat: float, lon: float, lib=math) -> float:\n",
    "    \"\"\"Converts degrees longitude to kilometers at a given latitude.\"\"\"\n",
    "    return lon * 111.32 * lib.cos(lib.radians(lat))\n",
    "\n",
    "\n",
    "def grid_coords(lat: float, lon: float, grid_size: float, lib=math) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the grid coordinate center for a given latitude and longitude and\n",
    "    grid size in kilometers.\n",
    "    \"\"\"\n",
    "    # latitude is always 111.32 km/degree\n",
    "    y_offset_km = 90 * 111.32  # deal with negative latitudes\n",
    "    y = lib.floor((lat_to_km(lat) + y_offset_km) / grid_size)\n",
    "\n",
    "    x_km_degree = lon_to_km(lat, 1, lib=lib)  # km/degree at this latitude\n",
    "    x_offset_km = 180 * x_km_degree  # deal with negative longitudes\n",
    "    x = lib.floor((lon_to_km(lat, lon, lib=lib) + x_offset_km) / grid_size)\n",
    "\n",
    "    y = (y * grid_size + grid_size / 2) / (111.32 * 180)\n",
    "    x = (x * grid_size + grid_size / 2) / (111.32 * 360)\n",
    "\n",
    "    return y, x\n",
    "\n",
    "\n",
    "grid_data = pd.DataFrame()\n",
    "\n",
    "for grid_size in range(10, 500, 10):\n",
    "    print(grid_size)\n",
    "    y, x = grid_coords(GeoName.lat, GeoName.lon, grid_size, lib=func)\n",
    "\n",
    "    counter = func.count(GeoName.osm_id).label(\"count\")\n",
    "    grid = func.cast(grid_size, Integer).label(\"grid_size\")\n",
    "    density = func.cast(counter / grid_size ** 2, Float).label(\"density\")\n",
    "    table_plus_grid = (\n",
    "        session.query(GeoName.osm_id)\n",
    "        .add_columns(y.label(\"y\"), x.label(\"x\"))\n",
    "        .add_columns(counter)\n",
    "        .add_columns(density)\n",
    "        .add_columns(func.sqrt(density).label(\"sqrt_density\"))\n",
    "        .add_columns(grid)\n",
    "        .add_columns(func.cast(grid_size ** 2, Integer).label(\"sq_km\"))\n",
    "        .group_by(x, y, grid)\n",
    "    )\n",
    "\n",
    "    grid_data = pd.concat(\n",
    "        [\n",
    "            grid_data,\n",
    "            pd.read_sql(table_plus_grid.statement, table_plus_grid.session.bind),\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "train = grid_data.sample(frac=0.8, random_state=0)\n",
    "val = grid_data.drop(train.index)\n",
    "test = grid_data.sample(frac=0.1, random_state=0)\n",
    "\n",
    "grid_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split data into features and labels\n",
    "features = [\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"sq_km\",\n",
    "]\n",
    "input_shape = len(features)\n",
    "predictions = [\"sqrt_density\"]\n",
    "output_shape = len(predictions)\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train[predictions]\n",
    "val_x = val[features]\n",
    "val_y = val[predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss_mse = logs.get(\"loss\")\n",
    "        val_loss_mse = logs.get(\"val_loss\")\n",
    "        self.losses.append(loss_mse)\n",
    "        self.val_losses.append(val_loss_mse)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.losses[2:], label=f\"loss ({self.losses[-1]:.4f})\", alpha=0.5)\n",
    "        plt.plot(self.val_losses[2:], label=f\"val_loss ({self.val_losses[-1]:.4f})\", alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.yscale(\"log\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "grid_model = Sequential(\n",
    "    [\n",
    "        Dense(256, input_shape=(input_shape,)),\n",
    "        Dense(128),\n",
    "        Dense(128),\n",
    "        Dense(128),\n",
    "        Dense(128),\n",
    "        Dense(output_shape, activation=\"linear\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    #clipval = tf.clip_by_value(y_pred, 0.0, 1.0)\n",
    "    penalty = 10\n",
    "    loss = tf.square(y_true - y_pred)\n",
    "    #loss += penalty * tf.where((y_pred < 0) | (y_pred > 1), tf.square(y_pred - 1), 0)\n",
    "\n",
    "    # Adding distribution penalty\n",
    "    distribution_penalty = tf.abs(1 / (y_pred / 500) - y_true)\n",
    "    loss += 0.2 * distribution_penalty\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "grid_model.compile(optimizer=\"adam\", loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Train model\n",
    "grid_model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=1000,\n",
    "    batch_size=10_000,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[plot_losses],\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(grid_model.predict(test[features]))\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    grid_data[prediction + \"_pred\"] = df[i]\n",
    "    grid_data[prediction + \"_pred_err\"] = abs(df[i] - grid_data[prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.yscale('linear')\n",
    "plt.hist(test[\"sqrt_density\"], 50, alpha=0.5, label='sqrt_density')\n",
    "#plt.hist([1/(x + 1) for x in range(len(test))], 50, alpha=0.5, label='myfunc')\n",
    "plt.hist(grid_data[\"sqrt_density_pred\"], 50, alpha=0.5, label='density_pred')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
