{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "from geo_dist_prep.data import GEONAMES_DB\n",
    "from geo_dist_prep.schemas.training_data import TrainingData\n",
    "\n",
    "# engine = create_engine(f\"sqlite:///../{GEONAMES_DB}\")\n",
    "engine = create_engine(f\"sqlite:///../.cache/geonames.db.britain.with-data\")\n",
    "\n",
    "Session = sessionmaker(bind=engine)\n",
    "session = Session()\n",
    "\n",
    "all_data_q = session.query(TrainingData)\n",
    "\n",
    "all_data = pd.read_sql(all_data_q.statement, all_data_q.session.bind)\n",
    "all_data.head()\n",
    "\n",
    "#all_data = all_data[all_data[\"distance\"] < 50]\n",
    "#all_data = all_data[all_data[\"distance\"] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shuff = all_data.sample(10000)\n",
    "\n",
    "count = 0\n",
    "\n",
    "# for _, row in shuff.iterrows():\n",
    "#     for x, y in [(row['x1'], row['y1']), (row['x2'], row['y2'])]:\n",
    "#         # if y < 0.574 or y > .578 or x < .798 or x > .802:\n",
    "#         #     continue\n",
    "\n",
    "#         count += 1\n",
    "\n",
    "plt.hist(all_data[\"distance\"], bins=1000)\n",
    "# plt.scatter(shuff['x1'], shuff['y1'], s=0.5, color='red')\n",
    "# plt.scatter(shuff['x2'], shuff['y2'], s=0.5, color='blue')\n",
    "# y*180, x*360, s=2, color='red' if count % 2 == 0 else 'blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "\n",
    "from geo_dist_prep.model.visualize import plot_losses\n",
    "\n",
    "# from geo_dist_prep.normalize import normalize_coords, denormalize_coords\n",
    "import numpy as np\n",
    "\n",
    "from math import sqrt, pow\n",
    "import math\n",
    "\n",
    "\n",
    "def denormalize_coords(y: float, x: float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Denormalize a latitude/longitude pair from [0, 1] to [-90, 90], [-180, 180].\n",
    "    \"\"\"\n",
    "    lat = y * 180.0 + 90\n",
    "    lon = x * 360.0 + 150\n",
    "\n",
    "    return lat, lon\n",
    "\n",
    "\n",
    "def lat_to_km(lat: float) -> float:\n",
    "    \"\"\"Converts degrees latitude to kilometers.\"\"\"\n",
    "    return lat * 111.32\n",
    "\n",
    "\n",
    "def lon_to_km(lat: float, lon: float, lib=math) -> float:\n",
    "    \"\"\"Converts degrees longitude to kilometers at a given latitude.\"\"\"\n",
    "    return lon * 111.32 * lib.cos(lib.radians(lat))\n",
    "\n",
    "\n",
    "def distance(y1, x1, y2, x2):\n",
    "    dist_y_sq = pow(y1 - y2, 2)\n",
    "    dist_x_sq = pow(x1 - x2, 2)\n",
    "\n",
    "    return sqrt(dist_x_sq + dist_y_sq)\n",
    "\n",
    "\n",
    "def km_dist_from_normalized(y1, x1, y2, x2):\n",
    "    \"\"\"grr...\"\"\"\n",
    "\n",
    "    lat1, lon1 = denormalize_coords(y1, x1)\n",
    "    lat2, lon2 = denormalize_coords(y2, x2)\n",
    "    xkm1 = lon_to_km(lat1, lon1)\n",
    "    xkm2 = lon_to_km(lat2, lon2)\n",
    "    ykm1 = lat_to_km(lat1)\n",
    "    ykm2 = lat_to_km(lat2)\n",
    "\n",
    "    return distance(xkm1, ykm1, xkm2, ykm2)\n",
    "\n",
    "\n",
    "all_data[\"sky_distance\"] = all_data.apply(\n",
    "    lambda row: km_dist_from_normalized(row[\"x1\"], row[\"y1\"], row[\"x2\"], row[\"y2\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "all_data[\"penalty\"] = np.maximum(all_data[\"distance\"] / all_data[\"sky_distance\"], 1.0)\n",
    "\n",
    "display(all_data[all_data[\"penalty\"] < 1.0])\n",
    "\n",
    "rules = {\n",
    "    \"unroutable\": (all_data[\"distance\"] < 0) | (all_data[\"routable\"] == False),\n",
    "    \"none\": (all_data[\"penalty\"] == 1) | ((0 < all_data[\"distance\"]) & (all_data[\"distance\"] < 2.5)),\n",
    "    \"10%\": (1 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 1.2),\n",
    "    \"20%\": (1.2 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 1.4),\n",
    "    \"50%\": (1.4 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 1.6),\n",
    "    \"70%\": (1.6 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 1.8),\n",
    "    \"90%\": (1.8 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 2.0),\n",
    "    \"125%\": (2.0 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 2.5),\n",
    "    \"175%\": (2.5 < all_data[\"penalty\"]) & (all_data[\"penalty\"] <= 3),\n",
    "    \"250%\": (all_data[\"penalty\"] > 3)  & (all_data[\"penalty\"] <= 4),\n",
    "    \"300%\": (all_data[\"penalty\"] > 4)  & (all_data[\"penalty\"] <= 5),\n",
    "    \"400%\": (all_data[\"penalty\"] > 5)  & (all_data[\"penalty\"] <= 6),\n",
    "    \"500%\": (all_data[\"penalty\"] > 5),\n",
    "}\n",
    "\n",
    "choices = rules.keys()\n",
    "conditions = rules.values()\n",
    "\n",
    "all_data[\"penalty_bin\"] = np.select(conditions, choices, default=\"other\")\n",
    "\n",
    "\n",
    "# from math import atan2, pi, cos, radians\n",
    "\n",
    "# def get_data(lat1, lat2, lon1, lon2):\n",
    "#     y1, x1 = normalize_coords(row[\"lat1\"], row[\"lon1\"])\n",
    "#     y2, x2 = normalize_coords(row[\"lat2\"], row[\"lon2\"])\n",
    "#     pos = (x1 * y1 + x2 * y2) / 2\n",
    "\n",
    "#     delta_lat = row[\"lat2\"] - row[\"lat1\"]\n",
    "#     delta_lon = row[\"lon2\"] - row[\"lon1\"]\n",
    "\n",
    "#     angle = atan2(delta_lat, delta_lon) + pi\n",
    "#     direction = (angle % (2 * pi)) / (2 * pi)\n",
    "\n",
    "#     x1_km = row[\"lon1\"] * 111.32 * cos(radians(row[\"lat1\"]))\n",
    "#     x2_km = row[\"lon2\"] * 111.32 * cos(radians(row[\"lat2\"]))\n",
    "#     y1_km = row[\"lat1\"] * 111.32\n",
    "#     y2_km = row[\"lat2\"] * 111.32\n",
    "#     dist = distance(x1_km, y1_km, x2_km, y2_km)\n",
    "\n",
    "#     return {\n",
    "#         \"pos\": pos,\n",
    "#         \"direction\": direction,\n",
    "#         \"sky_dist\": dist,\n",
    "#     }\n",
    "\n",
    "\n",
    "# for row in all_data.iterrows():\n",
    "#     d = normalize(row[1])\n",
    "#     row[d.keys()] = d.values()\n",
    "\n",
    "# all_data.apply(normalize, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# plt.hist( all_data[\"penalty\"].where(all_data[\"distance\"] >= 0), bins=50)\n",
    "# all_data[all_data['penalty'] == 1.0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = [\n",
    "    \"x1\",\n",
    "    \"y1\",\n",
    "    \"x2\",\n",
    "    \"y2\",\n",
    "    \"direction\",\n",
    "    \"sky_distance\",\n",
    "]\n",
    "input_shape = len(features)\n",
    "predictions = [\"penalty_bin_int\"]\n",
    "output_shape = len(choices)\n",
    "\n",
    "model = Sequential(\n",
    "    [\n",
    "        Dense(256, input_shape=(input_shape,)),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(output_shape, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "\n",
    "data = all_data.copy()\n",
    "\n",
    "category_to_int = {choice: i for i, choice in enumerate(choices)}\n",
    "\n",
    "data['penalty_bin_int'] = data['penalty_bin'].map(category_to_int)\n",
    "\n",
    "train = data.sample(frac=0.8, random_state=0)\n",
    "val = data.drop(train.index)\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_bin = to_categorical(train['penalty_bin_int'], num_classes=len(choices))\n",
    "val_bin = to_categorical(val['penalty_bin_int'], num_classes=len(choices))\n",
    "\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train_bin\n",
    "val_x = val[features]\n",
    "val_y = val_bin\n",
    "\n",
    "# Train model\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=300,\n",
    "    batch_size=1_000,\n",
    "    validation_data=(val_x, val_y),\n",
    "    callbacks=[plot_losses],\n",
    "    \n",
    ")\n",
    "\n",
    "# Evaluate model\n",
    "#model.evaluate(val_x, val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions\n",
    "val_predictions = model.predict(val_x)\n",
    "val_predictions = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Convert one-hot to integers\n",
    "val_y_int = np.argmax(val_y, axis=1)\n",
    "\n",
    "# Print classification report and confusion matrix\n",
    "print(classification_report(val_y_int, val_predictions, target_names=choices))\n",
    "print(confusion_matrix(val_y_int, val_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "southport = 26700924\n",
    "lat1, lon1 = -3.004175, 53.647599  # southport\n",
    "lat2, lon2 = -2.9979, 50.7463  # lytham\n",
    "# lat2, lon2 = -2.2522, 53.477 # manchester\n",
    "\n",
    "model.predict([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Let's try to predict the density of a grid location\n",
    "#\n",
    "from geo_dist_prep.schemas.geoname import GeoName\n",
    "from sqlalchemy import func, Integer, Float\n",
    "import math\n",
    "\n",
    "\n",
    "def lat_to_km(lat: float) -> float:\n",
    "    \"\"\"Converts degrees latitude to kilometers.\"\"\"\n",
    "    return lat * 111.32\n",
    "\n",
    "\n",
    "def lon_to_km(lat: float, lon: float, lib=math) -> float:\n",
    "    \"\"\"Converts degrees longitude to kilometers at a given latitude.\"\"\"\n",
    "    return lon * 111.32 * lib.cos(lib.radians(lat))\n",
    "\n",
    "\n",
    "def grid_coords(lat: float, lon: float, grid_size: float, lib=math) -> tuple[int, int]:\n",
    "    \"\"\"\n",
    "    Returns the grid coordinate center for a given latitude and longitude and\n",
    "    grid size in kilometers.\n",
    "    \"\"\"\n",
    "    # latitude is always 111.32 km/degree\n",
    "    y_offset_km = 90 * 111.32  # deal with negative latitudes\n",
    "    y = lib.floor((lat_to_km(lat) + y_offset_km) / grid_size)\n",
    "\n",
    "    x_km_degree = lon_to_km(lat, 1, lib=lib)  # km/degree at this latitude\n",
    "    x_offset_km = 180 * x_km_degree  # deal with negative longitudes\n",
    "    x = lib.floor((lon_to_km(lat, lon, lib=lib) + x_offset_km) / grid_size)\n",
    "\n",
    "    y = (y * grid_size + grid_size / 2) / (111.32 * 180)\n",
    "    x = (x * grid_size + grid_size / 2) / (111.32 * 360)\n",
    "\n",
    "    return y, x\n",
    "\n",
    "\n",
    "grid_data = pd.DataFrame()\n",
    "\n",
    "for grid_size in range(10, 500, 10):\n",
    "    print(grid_size)\n",
    "    y, x = grid_coords(GeoName.lat, GeoName.lon, grid_size, lib=func)\n",
    "\n",
    "    counter = func.count(GeoName.osm_id).label(\"count\")\n",
    "    grid = func.cast(grid_size, Integer).label(\"grid_size\")\n",
    "    density = func.cast(counter / grid_size ** 2, Float).label(\"density\")\n",
    "    table_plus_grid = (\n",
    "        session.query(GeoName.osm_id)\n",
    "        .add_columns(y.label(\"y\"), x.label(\"x\"))\n",
    "        .add_columns(counter)\n",
    "        .add_columns(density)\n",
    "        .add_columns(func.sqrt(density).label(\"sqrt_density\"))\n",
    "        .add_columns(grid)\n",
    "        .add_columns(func.cast(grid_size ** 2, Integer).label(\"sq_km\"))\n",
    "        .group_by(x, y, grid)\n",
    "    )\n",
    "\n",
    "    grid_data = pd.concat(\n",
    "        [\n",
    "            grid_data,\n",
    "            pd.read_sql(table_plus_grid.statement, table_plus_grid.session.bind),\n",
    "        ],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "train = grid_data.sample(frac=0.8, random_state=0)\n",
    "val = grid_data.drop(train.index)\n",
    "test = grid_data.sample(frac=0.1, random_state=0)\n",
    "\n",
    "grid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Split data into features and labels\n",
    "features = [\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"grid_size\",\n",
    "]\n",
    "input_shape = len(features)\n",
    "predictions = [\"density\"]\n",
    "output_shape = 10\n",
    "\n",
    "train_x = train[features]\n",
    "train_y = train[predictions]\n",
    "val_x = val[features]\n",
    "val_y = val[predictions]\n",
    "\n",
    "\n",
    "train_y_binned = pd.cut(train_y[\"density\"], bins=output_shape, labels=False)\n",
    "val_y_binned = pd.cut(val_y[\"density\"], bins=output_shape, labels=False)\n",
    "\n",
    "train_y_binned = to_categorical(train_y_binned, num_classes=output_shape)\n",
    "val_y_binned = to_categorical(val_y_binned, num_classes=output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, BatchNormalization, LeakyReLU\n",
    "\n",
    "\n",
    "class PlotLosses(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss_mse = logs.get(\"loss\")\n",
    "        val_loss_mse = logs.get(\"val_loss\")\n",
    "        self.losses.append(loss_mse)\n",
    "        self.val_losses.append(val_loss_mse)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        plt.plot(self.losses[2:], label=f\"loss ({self.losses[-1]:.4f})\", alpha=0.5)\n",
    "        plt.plot(self.val_losses[2:], label=f\"val_loss ({self.val_losses[-1]:.4f})\", alpha=0.5)\n",
    "        plt.legend()\n",
    "        plt.yscale(\"log\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot_losses = PlotLosses()\n",
    "\n",
    "grid_model = Sequential(\n",
    "    [\n",
    "        Dense(64, input_shape=(input_shape,)),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dense(output_shape, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "grid_model.compile(optimizer=\"adam\", loss='categorical_crossentropy')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Train model\n",
    "grid_model.fit(\n",
    "    train_x,\n",
    "    train_y_binned,\n",
    "    epochs=150,\n",
    "    batch_size=100_000,\n",
    "    validation_data=(val_x, val_y_binned),\n",
    "    callbacks=[plot_losses],\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(grid_model.predict(test[features]))\n",
    "\n",
    "# for i, prediction in enumerate(predictions):\n",
    "#     grid_data[prediction + \"_pred\"] = df[i]\n",
    "#     grid_data[prediction + \"_pred_err\"] = abs(df[i] - grid_data[prediction])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.yscale('log')\n",
    "#plt.hist(test[\"sqrt_density\"], 50, alpha=0.5, label='sqrt_density')\n",
    "#plt.hist([1/(x + 1) for x in range(len(test))], 50, alpha=0.5, label='myfunc')\n",
    "o = pd.get_dummies(pd.cut(test[\"density\"], bins=output_shape))\n",
    "\n",
    "#plt.plot(o)\n",
    "#plt.plot(df[2])\n",
    "#plt.hist(df)#, 5, alpha=0.5, label='predict', color='red')\n",
    "#plt.legend()\n",
    "display(o)\n",
    "display(df.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
